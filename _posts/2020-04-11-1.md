---
layout: post
title: "[1]Clustering Environment"
subtitle: "Linux Hadoop Docker RaspberryPi"
date: 2020-04-11 17:36:00 -0400
background: '/img/posts/03.jpg'
---
# Clustering Environment

Making a clustering environment pysique for fully distributed by the hadoop. After I terminate first step, I'll get a Spark for using a MLlib in the Clustering Environment. 

## 1. Pre-requirements 

### 1.1. Physical Requirements

* Wifi Router (Orange LiveBox)
* Raspberry Pi 4 (Ubuntu Server 18.04)
* Local Computer (OS X) 
* SD card



### 1.2. Software Environement

* openjdk-8-jdk
* hadoop-3.1.2



## 2. Preparation of images file : Ubuntu Server 18.04

Starting with the installation of Opertating System with Raspberry Pi 4. I'm able to use the Ubuntu Server version 18.04 based on the Raspberry Pi 4. 

Site : https://www.raspberrypi.org/downloads/

The ways to burn the OS to the SD card in OS X :

```shell
diskutil list # check SD card Mount

diskutil unmountDisk /dev/disk2 # unmount Disk

sudo diskutil eraseDisk FAT32 "BOOT" MBRFormat /dev/disk2 # for ubuntu 18.04 version, the format of SD card have to be FAT32

diskutil unmountDisk /dev/disk2

sudo sh -c 'gunzip -c ~/Downloads/ubuntu-18.04.4-preinstalled-server-arm64+raspi3.img.xz | sudo dd of=/dev/disk2 bs=32m'
```



Then, put the SD card into Raspberry Pi and go on head Ubuntu Settings. 



## 3. Ubuntu Settings 

### 3.1 Network

From Ubuntu Server 18.04 version, **netplan** organizes the network in the ubuntu server Machine. For the connection with Internet by wifi or by ethernet cable we need to configure a file.

```yaml
# /etc/netplan/50-cloud.init.yaml
network:
    version: 2
    renderer: networkd
    # Ways to be assigned ip by dhcp automatically in ethernet cable.
    ethernets:
        eth0:
            dhcp4: true
            optional: true
            
    # Ways to be assigned ip by static in wifis.
    wifis:
        wlan0:
                dhcp4: true
                dhcp6: false
                addresses: [192.168.2.5/24]
                gateway4: 192.168.2.1
                nameservers:
                        addresses: [8.8.8.8,8.8.4.4]
                access-points:
                        "RaspberryNetwork_Woo":
                                password: "admin"
```



### 3.2. General

```shell
# At first time, update and upgrade the package Manager(-apt)
sudo apt-get update && sudo apt-get upgrade

# take the root authority
su 
(put your new root passwd)
```



### 3.3. Firewall

```bash
# ftp,ssh,http,https open
sudo apt-get install ufw
sudo ufw enable
sudo ufw default deny
sudo ufw allow 21/tcp && sudo ufw allow 22/tcp && sudo ufw allow 80/tcp && sudo ufw allow 443/tcp

# If you want to open all ports 
sudo ufw disable
```



### 3.4. SSH

```bash
# Depending on your Raspbian OS, some of the version didn't install the ssh. 
sudo apt-get install ssh
sudo service ssh start
sudo systemctl enable ssh

# If you will use root account
sudo vi /etc/ssh/sshd_config
--(change)--
#PermitRootLogin without-password
--(to)--
PermitRootLogin yes
--(end)--

# If you can't access after reboot of device. If there isn't this file, make it 
sudo vi /etc/rc.local
--(insert)--
/bin/sh -e /etc/init.d/ssh start
--(end)--

# change the user (pi -> root)
su 
cd ~/
ssh-keygen -t rsa -P '' -f ~/.ssh/id_dsa
cd .ssh
cat id_dsa.pub >> authorized_keys
```



## 4. Docker Images

An example configuration scenario :

![그래프](https://github.com/WoohyunSHIN/woohyunshin.github.io/blob/master/img/input/Architecture_RaspberryPi.png?raw=true)

### 4.1. Docker installation

```shell
sudo apt-get update

sudo apt-get install docker-ce docker-ce-cli containerd.io

sudo docker run hello-world
```



### 4.2. Build Hadoop Base

There are a lot of different versions for Hadoop. In this time, we will use version Hadoop version 3.1.2 and we will build an image for other machines

```bash
sudo docker run --name hadoop_base -it ubuntu:18.04 /bin/bash

# In Container
cat /etc/issue # check release version
cd /home && mkdir data && cd data

# Package Manager update && upgrade
apt-get update
apt-get upgrade -y

# for IP check and network check
apt-get install net-tools
apt-get install iputils-ping
```



Before we are going to install the hadoop. It is worked based on Java. So we need to install  java.

```sh
# In Container
apt-get install software-properties-common 

apt-get install -y openjdk-8-jdk # install path : /usr/lib/jvm/java-8-openjdk-arm64

java -version # check
```



Path Setting : 

```shell
cd /home
mkdir -p soft/apache/hadoop
cd soft/apache/hadoop

# Using wget
apt-get install wget -y
wget http://archive.apache.org/dist/hadoop/common/hadoop-3.1.2/hadoop-3.1.2.tar.gz
tar -xvzf hadoop-3.1.2.tar.gz

# Environment Settings
apt-get install vim -y
vim ~/.bashrc


#Hadoop Config Path Setting
"""
export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-arm64
export HADOOP_HOME=/home/soft/apache/hadoop/hadoop-3.1.2
export HADOOP_CONFIG_HOME=$HADOOP_HOME/etc/hadoop
export PATH=$PATH:$HADOOP_HOME/bin
export PATH=$PATH:$HADOOP_HOME/sbin
"""

# Read the changes
source ~/.bashrc
```



In Hadoop, there are 3 modes depended on your environement : Standalon Mode, Pseudo-distributed Mode, and Fully distributed Mode.

1. Standalon Mode

> Standalone mode is also called local mode because it runs only locally where Hadoop is installed, and does not map-reduce in distributed environments because it does not drive the daemon provided by Hadoop.

2. Pseudo-distributed Mode

>Virtual distributed mode requires only one equipment, but all Hadoop configurations are made on that equipment. It is also called the imitation distributed mode (Pseudo-distributed Mode), and although it is an equipment, it returns to the full distributed mode.

3. Fully distributed Mode

> Fully distributed mode is when Hadoop is installed on multiple equipments and each equipment performs map-reduce. In order to achieve performance in the actual operating environment, it is necessary to set the mode to fully distributed mode.



This container will become a hadoop base image. So we don't config many things :

```shell
cd $HADOOP_HOME
mkdir tmp namenode datanode # Create 3 directories

cd $HADOOP_CONFIG_HOME
vim core-site.xml
"""
<configuration>
   <property>
       <name>hadoop.tmp.dir</name>
       <value>/home/soft/apache/hadoop/hadoop-3.1.2/tmp</value>
       <description>base temporary directories</description>
   </property>
   <property>
       <name>fs.default.name</name>
       <value>hdfs://master:9000</value>
       <final>true</final>
       <description>default file system name</description>
   </property>
</configuration>
"""

vim hdfs-site.xml
"""
<property>
	<name>dfs.replication</name>
	<value>4</value> # configure the number of duplication
	<final>true</final>
	<description>default block replication</description>
</property>
"""


vim hadoop-env.sh
"""
--(change)--
# export JAVA_HOME=
--(to)--
export JAVA_HOME=${JAVA_HOME}
--(end)--
"""

vim yarn-site.xml
"""
   <property>
       <name>yarn.nodemanager.aux-services</name>
       <value>mapreduce_shuffle</value>
   </property>
   <property>
       <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
       <value>org.apache.hadoop.mapred.ShuffleHandler</value>
   </property>
"""

vim mapred-site.xml
"""
  <property>
       <name>mapred.job.tracker</name>
       <value>master:9001</value>
       <description>host and port that the MapReduce job tracker runs</description>
   </property>  
"""

# Initialize NameNode
hadoop version
hadoop namenode -format

# Activate ssh
apt-get install ssh
service ssh start
systemctl enable ssh

# Key file generate 
cd ~
ssh-keygen -t rsa -P '' -f ~/.ssh/id_dsa
cd .ssh
cat id_dsa.pub >> authorized_keys
```



Return to Raspberry OS, we turn a container into an image and push it to docker hub for deployment. 

* username = docker hub ID

```shell
sudo docker commit hadoop_base {username}/hadoop_base

# Connect to my docker hub
sudo docker login 

sudo docker push {username}/hadoop_base
```



### 4.3. Docker Network

All data flows from Hadoop and Spark are made over the network. Therefore, the docker containers in each equipment should be able to communicate with each other. Creating a container allows users to use a default environment where they can communicate with the outside network through the dock0 bridge, but can write multiple network drivers at the user's choice. 



It will use MacVLAN to virtualize the host's network interface card to provide the container with a physical network environment. This enables communication with other devices connected to the network.



![그래프1](https://github.com/WoohyunSHIN/woohyunshin.github.io/blob/master/img/input/MacVLAN.png?raw=true)



In each Machine, we configure following command :

```shell
[Master01]
sudo docker network create -d macvlan --subnet=192.168.2.0/24 --ip-range=192.168.2.64/28 --gateway=192.168.2.1 -o macvlan_mode=bridge -o parent=eth0 master01_macvlan

[Master02]
sudo docker network create -d macvlan --subnet=192.168.2.0/24 --ip-range=192.168.2.128/28 --gateway=192.168.2.1 -o macvlan_mode=bridge -o parent=eht0 master02_macvlan

[Slave01]
sudo docker network create -d macvlan --subnet=192.168.2.0/24 --ip-range=192.168.2.192/28 --gateway=192.168.2.1 -o macvlan_mode=bridge -o parent=eth0 slave01_macvlan
```





